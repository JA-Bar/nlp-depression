{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prediction_suicidal.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JA-Bar/nlp-depression/blob/master/Prediction_suicidal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPduRyT8Tre7"
      },
      "source": [
        "#!gdown https://drive.google.com/uc?id=1FaDZvPevPZaw2IIUeNx9OwEmjL6Fe-Fe -O nlp.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IQm4R9CKmQI"
      },
      "source": [
        "import zipfile\n",
        "#!unzip '/content/nlp.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFNX1IJXVvSt"
      },
      "source": [
        "#!pip install -q tensorflow-text\n",
        "#!pip install -q tf-models-official\n",
        "#!pip install -q sklearn\n",
        "#!pip install tensorflow-text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJIvaOS-WB9D"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Activation, Dropout, Dense\n",
        "from keras.layers import Flatten, LSTM,SimpleRNN,GRU,RNN\n",
        "from keras.layers import GlobalMaxPooling1D\n",
        "from keras.models import Model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "from official.nlp import optimization \n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import pickle"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-WLnbOCZmX"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "    return sentence"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBRy2FYfKowK"
      },
      "source": [
        "def convert2embeding (word):\n",
        "  with open('/content/content/gdrive/MyDrive/Clasificador/nlp/tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "  X = []\n",
        "  sentences = list(word)\n",
        "  #print(sentences)\n",
        "  for sen in sentences:\n",
        "    X.append(preprocess_text(sen))\n",
        "  #print(X)\n",
        "  token_word=tokenizer.texts_to_sequences(X[0])\n",
        "  #print(token_word)\n",
        "  final=[]\n",
        "  for tok in token_word:\n",
        "    try:\n",
        "      final.append(tok[0])\n",
        "    except:\n",
        "      final.append(0)\n",
        "  #print(final)\n",
        "  maxlen = 32\n",
        "  final_word = pad_sequences([final], padding='post', maxlen=maxlen)\n",
        "  #print(final_word)\n",
        "  return final_word\n",
        "word2='Beatiful world baby'\n",
        "x=convert2embeding ([word2])\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLkrWY0vWdwp"
      },
      "source": [
        "glove_dataset_dep='/content/content/gdrive/MyDrive/Clasificador/nlp/glove_suicidal.h5'\n",
        "fasttext_dataset_dep='/content/content/gdrive/MyDrive/Clasificador/nlp/fasttext_dataset_hugging.h5'\n",
        "\n",
        "bert_sucidal = tf.saved_model.load('/content/content/gdrive/MyDrive/Clasificador/nlp/bert_suicidal')\n",
        "bert_sadness = tf.saved_model.load('/content/content/gdrive/MyDrive/Clasificador/nlp/bert_sadness')\n",
        "\n",
        "glove = keras.models.load_model(glove_dataset_dep)\n",
        "fasttext = keras.models.load_model(fasttext_dataset_dep)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IBt6CdzK-H2"
      },
      "source": [
        "def predictions (word2pred,glove_model,fasttext_model,bert_sucidal,bert_sadness):\n",
        "  final_pred=[]\n",
        "  bert_result=tf.sigmoid(bert_sucidal(tf.constant([word2pred])))\n",
        "  bert_result=bert_result.numpy()[0][0]\n",
        "  #print(bert_result)\n",
        "\n",
        "  bert_sad_result=tf.sigmoid(bert_sadness(tf.constant([word2pred])))\n",
        "  bert_sad_result=bert_sad_result.numpy()[0][0]\n",
        "  #print(bert_sad_result)\n",
        "\n",
        "  #print(bert_result >= .60)\n",
        "\n",
        "  if bert_result > .60:\n",
        "    #print('bert_sucidal')\n",
        "    final_pred.append([bert_result,1-bert_result])\n",
        "    final_pred=final_pred[0]\n",
        "  else:\n",
        "    #print('1')\n",
        "    convertion=convert2embeding([word2pred])\n",
        "    #print('2')\n",
        "    glove_pred = glove_model.predict(convertion)\n",
        "    #print('3')\n",
        "    fastext_pred = fasttext_model.predict(convertion)\n",
        "    #print('4')\n",
        "    for pred in range(len(glove_pred)):\n",
        "      #print('pred=', pred)\n",
        "      final_pred.append((glove_pred[pred]+fastext_pred[pred])/2)\n",
        "      #print('final_pred=', final_pred)\n",
        "      #print(\"final_pred = \",final_pred)\n",
        "      #print(\"glove_pred[pred] = \",glove_pred[pred])\n",
        "      #print(\"fastext_pred[pred] = \",fastext_pred[pred])\n",
        "    final_pred[0][1]=(final_pred[0][1]+bert_sad_result)/2\n",
        "    final_pred[0][0]=(final_pred[0][0])/1.1\n",
        "    final_pred=final_pred[0]\n",
        "  return final_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBnGCt-uV2Du"
      },
      "source": [
        "frase='i want to '\n",
        "x=predictions (frase,glove,fasttext,bert_sucidal,bert_sadness)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}